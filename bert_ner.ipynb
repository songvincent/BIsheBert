{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from net.crf import CRF\n",
    "import numpy as np\n",
    "# from sklearn.metrics import f1_score, classification_report\n",
    "# import config.args as args\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "# from transformers import  BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_Softmax(BertPreTrainedModel):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 num_tag):\n",
    "        super(Bert_Softmax, self).__init__(config)\n",
    "        # model = BertModel.from_pretrained('../bert-base-chinese/', config=config)\n",
    "        self.bert = BertModel.from_pretrained('../bert-base-chinese/', config=config) #.cuda()\n",
    "        # for p in self.bert.parameters():\n",
    "        #     p.requires_grad = False\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_tag)\n",
    "#         self.apply(self.init_bert_weights)\n",
    "\n",
    "#         self.crf = CRF(num_tag)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids,\n",
    "                attention_mask):\n",
    "        '''\n",
    "        self.bert\n",
    "        return tuple length = 2\n",
    "        tuple[0].shape [batch_size,seq_len+2(CLS+SEP),768]\n",
    "        '''\n",
    "        bert_encode, _ = self.bert(input_ids, attention_mask, token_type_ids)  ## \n",
    "        bert_encode = bert_encode.cuda()\n",
    "        output = self.classifier(bert_encode)  ##output_shape [batch_size,seq_len+2,num_tag]\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "#         output = torch.softmax(output,dim=2)  ##output_shape [batch_size,seq_len+2,num_tag]\n",
    "#         output = torch.argmax(output,dim=2)   ##output_shape [batch_size,seq_len+2]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE_Bert_Softmax(BertPreTrainedModel):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 num_tag):\n",
    "        super(RE_Bert_Softmax, self).__init__(config)\n",
    "        # model = BertModel.from_pretrained('../bert-base-chinese/', config=config)\n",
    "        self.bert = BertModel.from_pretrained('../bert-base-chinese/', config=config) #.cuda()\n",
    "        # for p in self.bert.parameters():\n",
    "        #     p.requires_grad = False\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_tag)\n",
    "#         self.apply(self.init_bert_weights)\n",
    "\n",
    "#         self.crf = CRF(num_tag)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids,\n",
    "                attention_mask):\n",
    "        '''\n",
    "        self.bert\n",
    "        return tuple length = 2\n",
    "        tuple[0].shape [batch_size,seq_len+2(CLS+SEP),768]\n",
    "        '''\n",
    "        _,bert_encode = self.bert(input_ids, attention_mask, token_type_ids)  ## [CLS]çš„output\n",
    "        bert_encode = bert_encode.cuda()\n",
    "#         bert_encode_re = [for be in bert_encode]\n",
    "        output = self.classifier(bert_encode)  ##output_shape [batch_size,seq_len+2,num_tag]\n",
    "#         output = F.log_softmax(output, dim=2)\n",
    "#         output = torch.softmax(output,dim=2)  ##output_shape [batch_size,seq_len+2,num_tag]\n",
    "#         output = torch.argmax(output,dim=2)   ##output_shape [batch_size,seq_len+2]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_loader import MyPro,convert_examples_to_features,create_batch_iter\n",
    "\n",
    "# # from bert_ner import Bert_Softmax\n",
    "# import torch\n",
    "# from transformers import BertConfig\n",
    "\n",
    "# label_list = ['O', 'B-Nh', 'I-Nh', 'B-NDR', 'I-NDR']\n",
    "# train_iter = create_batch_iter(\"train\",'../multi_data/multi_trainner.txt',16,label_list)\n",
    "\n",
    "# config = BertConfig.from_json_file('../bert-base-chinese/bert_config.json')\n",
    "# bert_softmax = Bert_Softmax(config,5).cuda()\n",
    "\n",
    "# # device = torch.device(if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# for step, batch in enumerate(train_iter):\n",
    "    \n",
    "#     input_ids, input_mask, segment_ids, label_ids = batch   ##(batch_size,512)\n",
    "#     input_ids = input_ids.cuda()\n",
    "#     input_mask = input_mask.cuda()\n",
    "#     segment_ids = segment_ids.cuda()\n",
    "#     label_ids = label_ids.cuda()\n",
    "#     out = bert_softmax(input_ids,segment_ids,input_mask)  #[batch_size,seq_len+2(512)]\n",
    "#     print(out.shape)\n",
    "#     print(out)\n",
    "# #     out = bert_softmax(input_ids,token_type_ids,attention_mask)  #[batch_size,seq_len+2(512)]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertConfig\n",
    "# config = BertConfig.from_json_file('../bert-base-chinese/bert_config.json')\n",
    "# model = BertModel.from_pretrained('../bert-base-chinese/', config=config)\n",
    "\n",
    "# model.parameters()\n",
    "\n",
    "# count = 0\n",
    "# for e in model.parameters():\n",
    "#     count += 1\n",
    "# #     print(type(e))\n",
    "#     print(e.shape)\n",
    "#     print(e)\n",
    "#     break\n",
    "\n",
    "# # for name, param in model.named_parameters():\n",
    "# #     print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
