{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import MyPro,convert_examples_to_features,create_batch_iter\n",
    "# from bert_ner import Bert_Softmax\n",
    "# from bert_ner import Bert_Softmax\n",
    "import torch\n",
    "from transformers import BertConfig,AdamW\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from Evalutor import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据entitys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n"
     ]
    }
   ],
   "source": [
    "with open('multi_data/entitys.json','r',encoding='utf-8') as f:\n",
    "    entitys = json.load(f)\n",
    "f.close()\n",
    "print(len(entitys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {}\n",
    "tag2id['O'] = 0\n",
    "tag2id['traffic_in'] = 1\n",
    "tag2id['sell_drugs_to'] = 2\n",
    "tag2id['provide_shelter_for'] = 3\n",
    "tag2id['posess'] = 4\n",
    "id2tag = ['O','traffic_in','sell_drugs_to','provide_shelter_for','posess']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JS_Model(BertPreTrainedModel):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 num_tag):\n",
    "        super(JS_Model, self).__init__(config)\n",
    "        self.bert = BertModel.from_pretrained('../bert-base-chinese/', config=config) #.cuda()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_tag)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids,\n",
    "                attention_mask,mode):\n",
    "\n",
    "        bert_encode = self.bert(input_ids, attention_mask, token_type_ids)  ## \n",
    "#         bert_encode = torch.tensor(bert_encode).cuda()\n",
    "        if mode ==\"ner\":\n",
    "            bert_encode0 = bert_encode[0].cuda()\n",
    "            output = self.classifier(bert_encode0)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "        elif mode == \"re\":\n",
    "            bert_encode1 = bert_encode[1].cuda()\n",
    "            output = self.classifier(bert_encode1)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model --->Shared Parameters\n",
    "'''\n",
    "\n",
    "\n",
    "config = BertConfig.from_json_file('../bert-base-chinese/bert_config.json')\n",
    "model = JS_Model(config,5).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "classifier.weight torch.Size([5, 768])\n",
      "classifier.bias torch.Size([5])\n",
      "******************************\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "classifier.weight torch.Size([5, 768])\n",
      "classifier.bias torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "### 冻结部分参数\n",
    "\n",
    "unfreeze_layers = ['layer.11','bert.pooler','classifier.']\n",
    " \n",
    "for name, param in model.named_parameters():\n",
    "    print(name,param.size())\n",
    "\n",
    "print(\"*\"*30)\n",
    "print('\\n')\n",
    "\n",
    "for name ,param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    for ele in unfreeze_layers:\n",
    "        if ele in name:\n",
    "            param.requires_grad = True\n",
    "            break\n",
    "#验证一下\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤掉requires_grad = False的参数\n",
    "'''\n",
    "  #### 优化器\n",
    "'''\n",
    "lr = 0.0001\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,weight_decay = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 涉及到NER的参数/变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SP_data_loader import create_batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataset\n",
    "label_list = ['O', 'B-Nh', 'I-Nh', 'B-NDR', 'I-NDR']\n",
    "train_iter = create_batch_iter(\"train\",'multi_data/multi_train_er.txt',12,label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_iter):\n",
    "#     count += 1\n",
    "    input_ids, input_mask, segment_ids, label_ids,others = batch   ##(batch_size,512)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[497, 547,   3],\n",
       "         [497, 380,   3],\n",
       "         [497, 142,   3],\n",
       "         [497, 147,   3],\n",
       "         [497, 483,   3],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[349,  13,   4],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[201,  13,   1],\n",
       "         [201, 927,   2],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[616, 969,   3],\n",
       "         [616, 420,   3],\n",
       "         [616, 673,   3],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[778,  47,   1],\n",
       "         [778,  31,   2],\n",
       "         [778, 231,   2],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[497, 865,   2],\n",
       "         [497,  47,   1],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[709, 705,   2],\n",
       "         [709, 309,   1],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[397, 459,   2],\n",
       "         [397,  47,   1],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[ 43, 973,   3],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[931, 309,   4],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[875, 309,   1],\n",
       "         [875,  31,   2],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[772, 431,   4],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irsdc/songwenhui/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "weight = [10.0] * len(label_list)   #生成一维\n",
    "weight[0] = 1\n",
    "weight = torch.tensor(weight).cuda()\n",
    "criterion = nn.NLLLoss(weight,size_average=False)      #定义的损失函数，softmax以后求损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 涉及到RE的参数/变量/函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('../bert-base-chinese/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('multi_data/multi_train_er.txt','r',encoding='utf-8') as f:\n",
    "    flines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_tag = [json.loads(line) for line in flines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_re_lines(out,label_ids,input_mask):\n",
    "    out = torch.argmax(out,dim= 2)\n",
    "    lines = []\n",
    "    for i in range(len(out)):\n",
    "        s_lines = []\n",
    "        label_true = [label_ids[i][j].item() for j in range(512) if input_mask[i][j] == 1 ]\n",
    "        y_pre = [out[i][j].tolist() for j in range(512) if input_mask[i][j] == 1 ]\n",
    "        y_pre = torch.tensor(y_pre)\n",
    "        valid_pos = torch.nonzero(y_pre).squeeze(1)\n",
    "\n",
    "        entitys = []\n",
    "#         entitys2 = []\n",
    "        flag = valid_pos[0]\n",
    "        start = flag\n",
    "        end=start\n",
    "        for vp in valid_pos[1:]:\n",
    "            if vp -1 == flag:\n",
    "                end = vp\n",
    "            elif start==end:\n",
    "                continue\n",
    "            else:\n",
    "                entity = tokenizer.decode(input_ids[i][start:end+1])\n",
    "                entity = entity.replace(' ','')\n",
    "                if entity == '':\n",
    "                    continue\n",
    "                entitys.append(entity)\n",
    "                start = vp\n",
    "\n",
    "            flag = vp\n",
    "        entity = tokenizer.decode(input_ids[i][start:valid_pos[-1]+1])\n",
    "        entity = entity.replace(' ','')\n",
    "        entitys.append(entity)   ### 最后一个实体\n",
    "#         print(entitys)\n",
    "        context = tokenizer.decode(input_ids[i][:len(y_pre)][1:-1])\n",
    "        context = context.replace(' ','')\n",
    "#         context = context[1:-1]\n",
    "\n",
    "\n",
    "        entitys = list(set(entitys))   ### 去重，无所谓顺序，因为两两实体都会进行组合\n",
    "#         e1_e2 = []\n",
    "        e_len = len(entitys)\n",
    "        for i in range(e_len):\n",
    "            e1 = entitys[i]\n",
    "            for j in range(e_len):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                e2 = entitys[j]\n",
    "                line = e1+'，'+e2 #+'。'+context\n",
    "                s_lines.append(line)\n",
    "        lines.append(s_lines)\n",
    "                \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids[0].shape\n",
    "\n",
    "# x = torch.tensor([0]).cuda()\n",
    "\n",
    "# len(segment_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = create_re_lines(out,label_ids,input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['威尔，冰毒',\n",
       "  '威尔，何某某',\n",
       "  '威尔，威尔远',\n",
       "  '威尔，甲基苯丙胺片剂',\n",
       "  '威尔，甲基安菲他明',\n",
       "  '威尔，姚某某',\n",
       "  '威尔，甲基苯丙胺',\n",
       "  '威尔，麻古',\n",
       "  '威尔，习某某',\n",
       "  '威尔，刘某',\n",
       "  '冰毒，威尔',\n",
       "  '冰毒，何某某',\n",
       "  '冰毒，威尔远',\n",
       "  '冰毒，甲基苯丙胺片剂',\n",
       "  '冰毒，甲基安菲他明',\n",
       "  '冰毒，姚某某',\n",
       "  '冰毒，甲基苯丙胺',\n",
       "  '冰毒，麻古',\n",
       "  '冰毒，习某某',\n",
       "  '冰毒，刘某',\n",
       "  '何某某，威尔',\n",
       "  '何某某，冰毒',\n",
       "  '何某某，威尔远',\n",
       "  '何某某，甲基苯丙胺片剂',\n",
       "  '何某某，甲基安菲他明',\n",
       "  '何某某，姚某某',\n",
       "  '何某某，甲基苯丙胺',\n",
       "  '何某某，麻古',\n",
       "  '何某某，习某某',\n",
       "  '何某某，刘某',\n",
       "  '威尔远，威尔',\n",
       "  '威尔远，冰毒',\n",
       "  '威尔远，何某某',\n",
       "  '威尔远，甲基苯丙胺片剂',\n",
       "  '威尔远，甲基安菲他明',\n",
       "  '威尔远，姚某某',\n",
       "  '威尔远，甲基苯丙胺',\n",
       "  '威尔远，麻古',\n",
       "  '威尔远，习某某',\n",
       "  '威尔远，刘某',\n",
       "  '甲基苯丙胺片剂，威尔',\n",
       "  '甲基苯丙胺片剂，冰毒',\n",
       "  '甲基苯丙胺片剂，何某某',\n",
       "  '甲基苯丙胺片剂，威尔远',\n",
       "  '甲基苯丙胺片剂，甲基安菲他明',\n",
       "  '甲基苯丙胺片剂，姚某某',\n",
       "  '甲基苯丙胺片剂，甲基苯丙胺',\n",
       "  '甲基苯丙胺片剂，麻古',\n",
       "  '甲基苯丙胺片剂，习某某',\n",
       "  '甲基苯丙胺片剂，刘某',\n",
       "  '甲基安菲他明，威尔',\n",
       "  '甲基安菲他明，冰毒',\n",
       "  '甲基安菲他明，何某某',\n",
       "  '甲基安菲他明，威尔远',\n",
       "  '甲基安菲他明，甲基苯丙胺片剂',\n",
       "  '甲基安菲他明，姚某某',\n",
       "  '甲基安菲他明，甲基苯丙胺',\n",
       "  '甲基安菲他明，麻古',\n",
       "  '甲基安菲他明，习某某',\n",
       "  '甲基安菲他明，刘某',\n",
       "  '姚某某，威尔',\n",
       "  '姚某某，冰毒',\n",
       "  '姚某某，何某某',\n",
       "  '姚某某，威尔远',\n",
       "  '姚某某，甲基苯丙胺片剂',\n",
       "  '姚某某，甲基安菲他明',\n",
       "  '姚某某，甲基苯丙胺',\n",
       "  '姚某某，麻古',\n",
       "  '姚某某，习某某',\n",
       "  '姚某某，刘某',\n",
       "  '甲基苯丙胺，威尔',\n",
       "  '甲基苯丙胺，冰毒',\n",
       "  '甲基苯丙胺，何某某',\n",
       "  '甲基苯丙胺，威尔远',\n",
       "  '甲基苯丙胺，甲基苯丙胺片剂',\n",
       "  '甲基苯丙胺，甲基安菲他明',\n",
       "  '甲基苯丙胺，姚某某',\n",
       "  '甲基苯丙胺，麻古',\n",
       "  '甲基苯丙胺，习某某',\n",
       "  '甲基苯丙胺，刘某',\n",
       "  '麻古，威尔',\n",
       "  '麻古，冰毒',\n",
       "  '麻古，何某某',\n",
       "  '麻古，威尔远',\n",
       "  '麻古，甲基苯丙胺片剂',\n",
       "  '麻古，甲基安菲他明',\n",
       "  '麻古，姚某某',\n",
       "  '麻古，甲基苯丙胺',\n",
       "  '麻古，习某某',\n",
       "  '麻古，刘某',\n",
       "  '习某某，威尔',\n",
       "  '习某某，冰毒',\n",
       "  '习某某，何某某',\n",
       "  '习某某，威尔远',\n",
       "  '习某某，甲基苯丙胺片剂',\n",
       "  '习某某，甲基安菲他明',\n",
       "  '习某某，姚某某',\n",
       "  '习某某，甲基苯丙胺',\n",
       "  '习某某，麻古',\n",
       "  '习某某，刘某',\n",
       "  '刘某，威尔',\n",
       "  '刘某，冰毒',\n",
       "  '刘某，何某某',\n",
       "  '刘某，威尔远',\n",
       "  '刘某，甲基苯丙胺片剂',\n",
       "  '刘某，甲基安菲他明',\n",
       "  '刘某，姚某某',\n",
       "  '刘某，甲基苯丙胺',\n",
       "  '刘某，麻古',\n",
       "  '刘某，习某某'],\n",
       " ['陈某某，李某某',\n",
       "  '陈某某，吴某某',\n",
       "  '陈某某，海洛因',\n",
       "  '陈某某，张某某',\n",
       "  '李某某，陈某某',\n",
       "  '李某某，吴某某',\n",
       "  '李某某，海洛因',\n",
       "  '李某某，张某某',\n",
       "  '吴某某，陈某某',\n",
       "  '吴某某，李某某',\n",
       "  '吴某某，海洛因',\n",
       "  '吴某某，张某某',\n",
       "  '海洛因，陈某某',\n",
       "  '海洛因，李某某',\n",
       "  '海洛因，吴某某',\n",
       "  '海洛因，张某某',\n",
       "  '张某某，陈某某',\n",
       "  '张某某，李某某',\n",
       "  '张某某，吴某某',\n",
       "  '张某某，海洛因'],\n",
       " ['曲某，宁村自己居住的屋内，容留王某某、林某某', '宁村自己居住的屋内，容留王某某、林某某，曲某'],\n",
       " ['于家房镇自己经营的手机店内容留蒋某某，姜某', '姜某，于家房镇自己经营的手机店内容留蒋某某'],\n",
       " ['冰毒，黄某', '冰毒，夏伟青', '黄某，冰毒', '黄某，夏伟青', '夏伟青，冰毒', '夏伟青，黄某'],\n",
       " ['冰毒，陈[UNK]海',\n",
       "  '冰毒，甲基苯丙胺',\n",
       "  '陈[UNK]海，冰毒',\n",
       "  '陈[UNK]海，甲基苯丙胺',\n",
       "  '甲基苯丙胺，冰毒',\n",
       "  '甲基苯丙胺，陈[UNK]海'],\n",
       " ['许某，甲基苯丙胺',\n",
       "  '许某，张某',\n",
       "  '许某，陈某',\n",
       "  '甲基苯丙胺，许某',\n",
       "  '甲基苯丙胺，张某',\n",
       "  '甲基苯丙胺，陈某',\n",
       "  '张某，许某',\n",
       "  '张某，甲基苯丙胺',\n",
       "  '张某，陈某',\n",
       "  '陈某，许某',\n",
       "  '陈某，甲基苯丙胺',\n",
       "  '陈某，张某'],\n",
       " ['陆某某，东新村10号楼408室其家中，容留陆海雷、黄某等人吸食甲基苯丙胺',\n",
       "  '东新村10号楼408室其家中，容留陆海雷、黄某等人吸食甲基苯丙胺，陆某某'],\n",
       " ['东升镇谭某某住宅，向吸毒人员黄某某贩卖毒品甲基苯丙胺1包。同年8月16日16时许，公安人员在东升镇某号房将被告人梁某某抓获归案，当场缴获毒品甲基苯丙胺，梁某某',\n",
       "  '梁某某，东升镇谭某某住宅，向吸毒人员黄某某贩卖毒品甲基苯丙胺1包。同年8月16日16时许，公安人员在东升镇某号房将被告人梁某某抓获归案，当场缴获毒品甲基苯丙胺'],\n",
       " ['甲基苯丙胺，周某', '周某，甲基苯丙胺'],\n",
       " ['冰毒，甲基苯丙胺', '冰毒，某某', '甲基苯丙胺，冰毒', '甲基苯丙胺，某某', '某某，冰毒', '某某，甲基苯丙胺'],\n",
       " ['陆某，麻古10包，总重量为187.01克，主要成分甲基苯丙胺，含量为15.54%；冰毒1包，重量为99.40克，主要成分甲基苯丙胺',\n",
       "  '麻古10包，总重量为187.01克，主要成分甲基苯丙胺，含量为15.54%；冰毒1包，重量为99.40克，主要成分甲基苯丙胺，陆某']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_trip(others):\n",
    "    valid_tris = []\n",
    "    for other in others:\n",
    "        valid_tri = []\n",
    "        for eel in other:\n",
    "\n",
    "            if eel[2] == 0:\n",
    "                break\n",
    "            valid_tri.append((entitys[eel[0]],entitys[eel[1]],eel[2].item()))\n",
    "        valid_tris.append(valid_tri)\n",
    "        \n",
    "    return valid_tris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tris = get_true_trip(others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 比较二给定的双实体 获得在原句中具有关系\n",
    "## bi --> (pre_e1,pre_e2)  \n",
    "## tris --> [(e1,e2,rela),...]\n",
    "def get_rela(bi,tris):\n",
    "    \n",
    "    bi = bi.split('，')\n",
    "#     print(bi)\n",
    "#     print(tris)\n",
    "    for tri in tris:\n",
    "#         print(tri)\n",
    "#         tri = tri.split(',')\n",
    "        if bi[0] == tri[0] and bi[1] == tri[1]:\n",
    "            return tri[2]\n",
    "        \n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## segment_ids 不需要更新 因为全为0\n",
    "def handle_ids_mask_seg(ids,mask,in_id):  ## in_id [CLS] 习某某，何某某。\n",
    "    length = len(in_id)\n",
    "    \n",
    "    in_id = torch.tensor(in_id).cuda()\n",
    "    new_ids = torch.cat((in_id,ids[1:]))[:512]\n",
    "    \n",
    "    _mask = torch.tensor([1]*length).cuda()\n",
    "    new_mask = torch.cat((_mask,mask[1:]))[:512]\n",
    "    \n",
    "#     _seg = torch.tensor([0]*length).cuda()\n",
    "#     new_seg = torch.cat((_seg))   seg不用变  因为全0\n",
    "    return new_ids,new_mask\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2.0\n",
      "0\n",
      "0\n",
      "3.0\n",
      "0\n",
      "0\n",
      "4.0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "5.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "6.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "7.0\n",
      "0\n",
      "0\n",
      "8.0\n",
      "0\n",
      "0\n",
      "9.0\n",
      "0\n",
      "4\n",
      "10.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "11.0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "assert len(lines) == len(true_tris)\n",
    "\n",
    "length = len(lines)\n",
    "for i in range(length):\n",
    "    print(float(i))\n",
    "    input_id = input_ids[i]\n",
    "    input_m = input_mask[i]\n",
    "    line = lines[i]\n",
    "    true_tri = true_tris[i]\n",
    "#     print(true_tri)\n",
    "    for l in line:\n",
    "#         print(true_tri)\n",
    "        label = get_rela(l,true_tri)\n",
    "#         if label != 0:\n",
    "        print(label)\n",
    "        tokens = [\"[CLS]\"] + list(l+'。')\n",
    "        in_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "#             print(l)\n",
    "#             print(tokens)\n",
    "#             print(in_id)\n",
    "        assert len(tokens) == len(in_id)\n",
    "        new_id,new_mask = handle_ids_mask_seg(input_id,input_m,in_id)\n",
    "\n",
    "        assert Counter(new_id.tolist())[0] == Counter(new_mask.tolist())[0]\n",
    "            \n",
    "#             break\n",
    "#         print(label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(new_id.tolist())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(new_mask.tolist())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('习某某', '何某某', 3), ('习某某', '姚某某', 3), ('习某某', '刘某', 3)],\n",
       " [('张某某', '海洛因', 1), ('张某某', '吴某某', 2), ('张某某', '陈某某', 2), ('张某某', '李某某', 2)],\n",
       " [('曲某', '王某某', 3), ('曲某', '林某某', 3)],\n",
       " [('姜某', '蒋某某', 3)],\n",
       " [('黄某', '夏伟青', 3)],\n",
       " [('陈X海', '冰毒', 4)],\n",
       " [('张某', '许某', 3), ('张某', '陈某', 3)],\n",
       " [('陆某某', '黄某', 3), ('陆某某', '陆海雷', 3)],\n",
       " [('梁某某', '甲基苯丙胺', 1), ('梁某某', '黄某某', 2)],\n",
       " [('周某', '甲基苯丙胺', 4)],\n",
       " [('崔某某', '冰毒', 4)],\n",
       " [('陆某', '甲基苯丙胺', 4)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_tris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1140.1543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(824.2770, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(752.8044, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(694.7253, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(521.0657, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(464.6865, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(350.1262, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(325.3911, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(347.9478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(371.6476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(209.3430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(211.1847, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(173.2633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(129.3235, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(180.1134, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(151.2044, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(161.8375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(121.6269, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(90.0760, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(98.1096, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(71.3061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(55.0254, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(92.6505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(79.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(47.2936, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(66.0979, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(46.1682, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(64.9221, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(62.4536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(41.2646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(44.6212, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(52.1743, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(25.7649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(67.8210, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(19.7693, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "0：结束\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1):\n",
    "    count = 0\n",
    "    if (e+1)%2 == 0:\n",
    "        lr = lr/2\n",
    "        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,weight_decay = 0.1)\n",
    "    \n",
    "    for step, batch in enumerate(train_iter):\n",
    "        count += 1\n",
    "        \n",
    "        input_ids, input_mask, segment_ids, label_ids,others = batch   ##(batch_size,512)\n",
    "        \n",
    "        input_ids = input_ids.cuda()\n",
    "        input_mask = input_mask.cuda()\n",
    "        segment_ids = segment_ids.cuda()\n",
    "        label_ids = label_ids.cuda()\n",
    "        out = model(input_ids,segment_ids,input_mask,\"ner\")  #[batch_size,seq_len+2(512)]\n",
    "        \n",
    "        '''\n",
    "        计算loss\n",
    "        '''\n",
    "\n",
    "        out1 = out.view(-1, out.size(2))\n",
    "        res1 = label_ids[0]\n",
    "        for i in range(1,len(label_ids)):\n",
    "            res1 = torch.cat((res1,label_ids[i]))\n",
    "        loss = criterion(out1,res1)\n",
    "        loss = loss/len(out)\n",
    "        print(loss)\n",
    "        if loss <20.0:\n",
    "            \n",
    "#             lines = create_re_lines(out,label_ids,input_mask)\n",
    "            break\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        产生re数据\n",
    "        '''\n",
    "        \n",
    "#         lines = create_re_lines(out,label_ids,input_mask)\n",
    "#         out = torch.argmax(out,dim= 2)\n",
    "#         lines = []\n",
    "#         for i in range(len(out)):\n",
    "#             label_true = [label_ids[i][j].item() for j in range(512) if input_mask[i][j] == 1 ]\n",
    "#             y_pre = [out[i][j].tolist() for j in range(512) if input_mask[i][j] == 1 ]\n",
    "#             y_pre = torch.tensor(y_pre)\n",
    "#             valid_pos = torch.nonzero(y_pre).squeeze(1)\n",
    "\n",
    "#             entitys = []\n",
    "#     #         entitys2 = []\n",
    "#             flag = valid_pos[0]\n",
    "#             start = flag\n",
    "#             for vp in valid_pos[1:]:\n",
    "#                 if vp -1 == flag:\n",
    "#                     end = vp\n",
    "#                 else:\n",
    "#                     entity = tokenizer.decode(input_ids[i][start:end+1])\n",
    "#                     entity = entity.replace(' ','')\n",
    "#                     entitys.append(entity)\n",
    "#                     start = vp\n",
    "\n",
    "#                 flag = vp\n",
    "#             entity = tokenizer.decode(input_ids[i][start:valid_pos[-1]+1])\n",
    "#             entity = entity.replace(' ','')\n",
    "#             entitys.append(entity)   ### 最后一个实体\n",
    "#     #         print(entitys)\n",
    "#             context = tokenizer.decode(input_ids[i][:len(y_pre)][1:-1])\n",
    "#             context = context.replace(' ','')\n",
    "#     #         context = context[1:-1]\n",
    "\n",
    "\n",
    "#             entitys = list(set(entitys))   ### 去重，无所谓顺序，因为两两实体都会进行组合\n",
    "#     #         e1_e2 = []\n",
    "#             e_len = len(entitys)\n",
    "#             for i in range(e_len):\n",
    "#                 e1 = entitys[i]\n",
    "#                 for j in range(e_len):\n",
    "#                     if i == j:\n",
    "#                         continue\n",
    "#                     e2 = entitys[j]\n",
    "#                     line = e1+'，'+e2+'。'+context\n",
    "#                     lines.append(line)\n",
    "#         break\n",
    "        \n",
    "#         if count %5 == 0:\n",
    "#             print('loss: ',loss)\n",
    "#             out2 = torch.argmax(out,dim=2)\n",
    "#             batch_precision,batch_recall,batch_f1 = evaluate(out2,label_ids,input_mask)\n",
    "#             print('batch_precision:%.4f  batch_recall:%.4f  batch_f1: %.4f' %(batch_precision,batch_recall,batch_f1)) \n",
    "            \n",
    "#             bor_precision, bor_recall,bor_f1 = evaluate_bor(out2,label_ids,input_mask)\n",
    "#             print('bor_precision:%.4f  bor_recall:%.4f  bor_f1: %.4f' %(bor_precision,bor_recall,bor_f1)) \n",
    "#         print(lines)\n",
    "#         break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(str(e)+'：结束')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tris = []\n",
    "for other in others:\n",
    "    valid_tri = []\n",
    "    for eel in other:\n",
    "        \n",
    "        if eel[2] == 0:\n",
    "            break\n",
    "        valid_tri.append((entitys[eel[0]],entitys[eel[1]],id2tag[eel[2]]))\n",
    "    valid_tris.append(valid_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('廖某甲', '甲基苯丙胺', 'posess')],\n",
       " [('谷某', '林某', 'provide_shelter_for'), ('谷某', '潘某甲', 'provide_shelter_for')],\n",
       " [('吴某', '甲基苯丙胺', 'posess'),\n",
       "  ('乌立', '冰毒', 'traffic_in'),\n",
       "  ('乌立', '吴某', 'sell_drugs_to')],\n",
       " [('吴某某', '甲基苯丙胺', 'traffic_in'), ('吴某某', '王良进', 'sell_drugs_to')],\n",
       " [('宋某某', '薛某某', 'provide_shelter_for'),\n",
       "  ('宋某某', '丁某某', 'provide_shelter_for'),\n",
       "  ('宋某某', '薛某某', 'provide_shelter_for'),\n",
       "  ('宋某某', '丁某某', 'provide_shelter_for')],\n",
       " [('毛某某', '冰毒', 'posess')],\n",
       " [('路某某', '海洛因', 'traffic_in')],\n",
       " [('杨某', '杨某某', 'provide_shelter_for'),\n",
       "  ('杨某', '张某', 'provide_shelter_for'),\n",
       "  ('杨某', '刘某', 'provide_shelter_for')],\n",
       " [('赵国通', '海洛因', 'traffic_in'), ('赵国通', '乔某', 'sell_drugs_to')],\n",
       " [('官某', '冰毒', 'posess')],\n",
       " [('刘某某', '邓某', 'provide_shelter_for'), ('刘某某', '宋某', 'provide_shelter_for')],\n",
       " [('徐a', '张a', 'provide_shelter_for'),\n",
       "  ('徐a', '樊a', 'provide_shelter_for'),\n",
       "  ('徐a', '肖a', 'provide_shelter_for'),\n",
       "  ('徐a', '刘a', 'provide_shelter_for'),\n",
       "  ('徐a', '唐a', 'provide_shelter_for'),\n",
       "  ('徐a', '王a', 'provide_shelter_for')]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_tris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[990,  13,   4],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[500, 712,   3],\n",
       "         [500, 494,   3],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[814,  13,   4],\n",
       "         [947,  47,   1],\n",
       "         [947, 814,   2],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[809,  13,   1],\n",
       "         [809, 330,   2],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[ 43, 478,   3],\n",
       "         [ 43, 265,   3],\n",
       "         [ 43, 478,   3],\n",
       "         [ 43, 265,   3],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[886,  47,   4],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[204, 309,   1],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[656, 520,   3],\n",
       "         [656, 497,   3],\n",
       "         [656, 917,   3],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[210, 309,   1],\n",
       "         [210, 763,   2],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[736,  47,   4],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[181,  89,   3],\n",
       "         [181, 849,   3],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[733, 290,   3],\n",
       "         [733, 729,   3],\n",
       "         [733, 195,   3],\n",
       "         [733, 967,   3],\n",
       "         [733, 296,   3],\n",
       "         [733, 550,   3],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'甲基苯丙胺'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = others[0][0]\n",
    "\n",
    "entitys[a[1]]\n",
    "# entitys[a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'廖某甲'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entitys[a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = create_re_lines(out,label_ids,input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'冰毒，林某。公诉机关指控：2016年1月2月间，被告人谷某在其位于南京市高淳区[UNK][UNK]镇[UNK][UNK]号的住所内，先后3次容留林某、潘某甲吸食毒品甲基苯丙胺（冰毒）。具体事实如下：'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.tensor([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    2,  101, 7355,  705, 2356,  782, 3696, 3466, 2175, 7368, 2900,\n",
       "        2971, 8038, 7355,  705, 2356, 1062, 2128, 2229,  754,  123,  121,  122,\n",
       "         127, 2399,  129, 3299,  123,  125, 3189,  677, 1286,  130, 3198, 6387,\n",
       "        8024, 2190, 3315, 2356, 4818, 4767, 7252, 4371, 3636, 2255, 1920, 6983,\n",
       "        2421, 2809, 6121, 3926, 3389,  818, 1218, 3198, 8024, 1762, 6983, 2421,\n",
       "         122,  125,  122,  123, 2791, 2831, 5815, 6158, 1440,  782, 2445, 3378,\n",
       "        4508, 8024,  794, 1071, 7390, 6716, 3025, 2372, 4638,  671,  702, 5905,\n",
       "        5682, 2357, 3160, 1169, 5541, 1259, 7027, 3389, 5815,  671, 1749, 2501,\n",
       "        1848, 3160, 2207, 4665, 8024, 1079, 3300, 5310, 3253, 4307, 4289, 4542,\n",
       "         849, 3681, 1501, 1112, 7028,  124,  119,  125, 1046, 8039,  671, 5905,\n",
       "        5682, 2207, 7188, 4665, 1079, 3300, 4500, 5905, 5682, 1848, 3160, 6150,\n",
       "        1259, 6163, 4638, 4542,  849, 3681, 1501, 7578, 5108, 4307, 5790,  709,\n",
       "         122,  124,  128, 7578, 8020, 5273, 5682,  122,  124,  126, 7578,  510,\n",
       "        5344, 5682,  123, 7578, 8021, 8024, 1112, 7028, 6369,  122,  124,  119,\n",
       "         124, 1046, 1350,  676, 2476, 7213, 6121, 1305,  510, 4385, 7032,  782,\n",
       "        3696, 2355,  126,  126,  121,  121, 1039,  511, 1762, 2791, 7313,  691,\n",
       "        1298, 6235,  671, 2414, 1928, 3385,  677, 3389, 5815,  671, 6851, 3209,\n",
       "        2196, 1366, 6150, 8024, 6150, 1079, 6163, 3300,  124,  702, 3783, 3300,\n",
       "        2544, 7030, 4542,  849, 3681, 1501, 4638, 2196, 1366, 6150,  511, 1762,\n",
       "        2791, 7313, 1298,  904,  671, 5763, 1126,  677, 3389, 5815, 4635, 5291,\n",
       "         671, 2476, 8024, 5291,  677, 3300, 5310, 3253, 4307, 4542,  849, 3681,\n",
       "        1501,  124, 2207, 1779, 8024, 1112, 7028, 6369,  122, 1046, 8039,  671,\n",
       "        7378, 1169, 2207, 5763, 3344, 8024, 3344, 7027, 3300, 4542,  849, 3681,\n",
       "        1501, 8024, 1439, 3890,  860, 4307, 8024, 1112, 7028, 6369,  127,  119,\n",
       "         125, 1046, 1350, 1429, 3681, 2339, 1072,  671, 1947,  511, 5307, 7063,\n",
       "        2137, 8024, 5373, 5815, 4638, 4542,  849, 3681, 1501, 1772, 3466, 1139,\n",
       "        4508, 1825, 5738,  688, 5542, 2768, 1146,  511,  102,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((a,input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
