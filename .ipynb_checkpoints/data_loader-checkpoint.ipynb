{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "# from util.Logginger import init_logger\n",
    "# import config.args as args\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X, y, valid_size=0.2, random_state=2020, shuffle=True):\n",
    "    \"\"\"\n",
    "    训练集验证集分割\n",
    "    :param X: sentences\n",
    "    :param y: labels\n",
    "    :param random_state: 随机种子\n",
    "    \"\"\"\n",
    "#     logger.info('Train val split')\n",
    "\n",
    "    data = []\n",
    "    for data_x, data_y in tqdm(zip(X, y), desc='Merge'):\n",
    "        data.append((data_x, data_y))\n",
    "    del X, y\n",
    "\n",
    "    N = len(data)\n",
    "    test_size = int(N * valid_size)\n",
    "\n",
    "    if shuffle:\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(data)\n",
    "\n",
    "    valid = data[:test_size]\n",
    "    train = data[test_size:]\n",
    "\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sent2char(line):\n",
    "    \"\"\"\n",
    "    句子处理成单词\n",
    "    :param line: 原始行\n",
    "    :return: 单词， 标签\n",
    "    \"\"\"\n",
    "    res = line.strip('\\n').split()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"创建一个输入实例\n",
    "        Args:\n",
    "            guid: 每个example拥有唯一的id\n",
    "            text_a: 第一个句子的原始文本，一般对于文本分类来说，只需要text_a\n",
    "            text_b: 第二个句子的原始文本，在句子对的任务中才有，分类问题中为None\n",
    "            label: example对应的标签，对于训练集和验证集应非None，测试集为None\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeature(object):\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, output_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.output_mask = output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"数据预处理的基类，自定义的MyPro继承该类\"\"\"\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"读取训练集 Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"读取验证集 Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"读取标签 Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_json(cls, input_file):\n",
    "        with open(input_file, \"r\", encoding='utf-8') as fr:\n",
    "            lines = []\n",
    "            for line in fr:\n",
    "                _line = line.strip('\\n')\n",
    "                lines.append(_line)\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPro(DataProcessor):\n",
    "    \"\"\"将数据构造成example格式\"\"\"\n",
    "    \"\"\"输入为json文件，每一行一个dumps后的字符串\"\"\"\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for i, line in enumerate(lines):\n",
    "            guid = \"%s-%d\" % (set_type, i)\n",
    "            line = json.loads(line)\n",
    "            text_a = line[\"source\"] ##文字内容\n",
    "            label = line[\"target\"]  ##list\n",
    "            #assert len(label) == len(text_a.split())  利用split是无法进行中文分词的，需要用到list()函数\n",
    "            assert len(label) == len(list(label))\n",
    "            example = InputExample(guid=guid, text_a=text_a, label=label)\n",
    "            examples.append(example)\n",
    "        return examples\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        lines = self._read_json(data_dir)\n",
    "        examples = self._create_example(lines, \"train\")\n",
    "        return examples\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        lines = self._read_json(data_dir)\n",
    "        examples = self._create_example(lines, \"dev\")\n",
    "        return examples\n",
    "\n",
    "    def get_labels(self):\n",
    "        return args.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    # 标签转换为数字\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    # load sub_vocab\n",
    "    sub_vocab = {}\n",
    "#     with open(VOCAB_FILE, 'r') as fr:\n",
    "#         for line in fr:\n",
    "#             _line = line.strip('\\n')\n",
    "#             if \"##\" in _line and sub_vocab.get(_line) is None:\n",
    "#                 sub_vocab[_line] = 1\n",
    "\n",
    "    features = []\n",
    "    for ex_index, example in enumerate(examples):\n",
    "#         tokens_a = tokenizer.tokenize(example.text_a)  这种分词，会将“2015”分为一个词\n",
    "                                                        ## encode方法 则是在上述的基础上，前后加结束标志（[CLS][SEP]）\n",
    "        tokens_a = list(example.text_a)\n",
    "#         print(tokens_a)\n",
    "        labels = example.label\n",
    "#         print(tokens_a)\n",
    "#         print(labels)\n",
    "        assert len(tokens_a)== len(labels)\n",
    "        # labels = example.label.split()\n",
    "        \n",
    "        if len(tokens_a)==0 or len(labels)==0:\n",
    "            continue\n",
    "            \n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length-2)]\n",
    "            labels = labels[:(max_seq_length-2)]\n",
    "        \n",
    "        \n",
    "        assert len(tokens_a)== len(labels)  ### 第一个监测点\n",
    "        \n",
    "        # ----------------处理source--------------\n",
    "        ## 句子首尾加入标示符\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        ## 词转换成数字\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        # ---------------处理target----------------\n",
    "        ## Notes: label_id中不包括[CLS]和[SEP]\n",
    "        label_id = [label_map[l] for l in labels]\n",
    "        \n",
    "        label_id = [0] + label_id + [0]\n",
    "        label_padding = [0] * (max_seq_length-len(label_id))  ## 由 -1 --> 0\n",
    "        label_id += label_padding\n",
    "\n",
    "        ## output_mask用来过滤bert输出中sub_word的输出,只保留单词的第一个输出(As recommended by jocob in his paper)\n",
    "        ## 此外，也是为了适应crf\n",
    "        output_mask = [0 if sub_vocab.get(t) is not None else 1 for t in tokens_a]\n",
    "        output_mask = [0] + output_mask + [0]\n",
    "        output_mask += padding\n",
    "\n",
    "        # ----------------处理后结果-------------------------\n",
    "        # for example, in the case of max_seq_length=10:\n",
    "        # raw_data:          春 秋 忽 代 谢le\n",
    "        # token:       [CLS] 春 秋 忽 代 谢 ##le [SEP]\n",
    "        # input_ids:     101 2  12 13 16 14 15   102   0 0 0\n",
    "        # input_mask:      1 1  1  1  1  1   1     1   0 0 0\n",
    "        # label_id:          T  T  O  O  O\n",
    "        # output_mask:     0 1  1  1  1  1   0     0   0 0 0\n",
    "        # --------------看结果是否合理------------------------\n",
    "\n",
    "        feature = InputFeature(input_ids=input_ids,\n",
    "                               input_mask=input_mask,\n",
    "                               segment_ids=segment_ids,\n",
    "                               label_id=label_id,\n",
    "                               output_mask=output_mask)\n",
    "        features.append(feature)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_iter(mode,data_dir,train_batch_size,label_list):\n",
    "    \"\"\"构造迭代器\"\"\"\n",
    "#     processor, tokenizer = init_params()\n",
    "    processor = MyPro()\n",
    "    tokenizer = BertTokenizer('../bert-base-chinese/vocab.txt')\n",
    "    if mode == \"train\":\n",
    "        examples = processor.get_train_examples(data_dir)\n",
    "\n",
    "#         num_train_steps = int(\n",
    "#             len(examples) /train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
    "\n",
    "        batch_size = train_batch_size\n",
    "\n",
    "#         logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "    elif mode == \"dev\":\n",
    "        examples = processor.get_dev_examples(args.data_dir)\n",
    "        batch_size = args.eval_batch_size\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode %s\" % mode)\n",
    "\n",
    "#     label_list = processor.get_labels()\n",
    "\n",
    "    # 特征\n",
    "    features = convert_examples_to_features(examples, label_list, 512, tokenizer)\n",
    "\n",
    "#     logger.info(\"  Num examples = %d\", len(examples))\n",
    "#     logger.info(\"  Batch size = %d\", batch_size)\n",
    "\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "#     for \n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "#     all_output_mask = torch.tensor([f.output_mask for f in features], dtype=torch.long)\n",
    "\n",
    "    # 数据集\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)# , all_output_mask)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        sampler = RandomSampler(data)\n",
    "    elif mode == \"dev\":\n",
    "        sampler = SequentialSampler(data)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode %s\" % mode)\n",
    "\n",
    "    # 迭代器\n",
    "    iterator = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        return iterator\n",
    "#         return iterator, num_train_steps\n",
    "    elif mode == \"dev\":\n",
    "        return iterator\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode %s\" % mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = ['O', 'B-Nh', 'I-Nh', 'B-NDR', 'I-NDR']\n",
    "# train_iter = create_batch_iter(\"train\",'../multi_data/multi_trainner.txt',32,label_list)\n",
    "\n",
    "# for step, batch in enumerate(train_iter):\n",
    "#     input_ids, input_mask, segment_ids, label_ids = batch\n",
    "#     print(input_ids.shape)   ##(batch_size,512)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mypro = MyPro()\n",
    "# in_exas =  mypro.get_train_examples('../multi_data/multi_trainner.txt')\n",
    "\n",
    "# raw_label = ['Nh','NDR']\n",
    "# label_list = ['O']\n",
    "# for rl in raw_label:\n",
    "#     label_list.append('B-'+rl)\n",
    "#     label_list.append('I-'+rl)\n",
    "# print(label_list)   \n",
    "# # for ie in in_exas:\n",
    "# #     print(ie.label)\n",
    "# #     break\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer('../bert-base-chinese/vocab.txt')\n",
    "\n",
    "# #convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "\n",
    "# features = convert_examples_to_features(in_exas,label_list,512,tokenizer)\n",
    "\n",
    "# # features[0]\n",
    "\n",
    "# # input_ids=input_ids,\n",
    "# # input_mask=input_mask, attention_mask\n",
    "# # segment_ids=segment_ids,tokrn_type_id\n",
    "# # label_id=label_id,\n",
    "# # output_mask=output_mask\n",
    "\n",
    "# tokenizer = BertTokenizer('../bert-base-chinese/vocab.txt')\n",
    "\n",
    "# tokenizer.encode('2015年2月16日我去北京')\n",
    "\n",
    "# tokenizer.tokenize('2015年2月16日我去北京')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer('../bert-base-chinese/vocab.txt')\n",
    "\n",
    "# #convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "\n",
    "# features = convert_examples_to_features(in_exas[1:2],label_list,512,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
