{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://huggingface.co/transformers/model_doc/bert.html\n",
    "## https://huggingface.co/transformers/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_loader 的MyPro加载数据\n",
    "### bert_ner 中 有模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检测模型使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import MyPro,convert_examples_to_features,create_batch_iter\n",
    "from bert_ner import Bert_Softmax\n",
    "# from bert_ner import Bert_Softmax\n",
    "import torch\n",
    "from transformers import BertConfig,AdamW\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from Evalutor import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evalutor import evaluate_bor#evaluate_bor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-Nh', 'I-Nh', 'B-NDR', 'I-NDR']\n",
    "train_iter = create_batch_iter(\"train\",'../multi_data/multi_trainner.txt',12,label_list)\n",
    "\n",
    "config = BertConfig.from_json_file('../bert-base-chinese/bert_config.json')\n",
    "bert_softmax = Bert_Softmax(config,5).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irsdc/songwenhui/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "weight = [10.0] * len(label_list)   #生成一维\n",
    "\n",
    "weight[0] = 1\n",
    "\n",
    "weight = torch.tensor(weight).cuda()\n",
    "\n",
    "criterion = nn.NLLLoss(weight,size_average=False)      #定义的损失函数，softmax以后求损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_optimizer = bert_softmax.named_parameters()\n",
    "\n",
    "# param_optimizer = list(bert_softmax.named_parameters())\n",
    "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "# optimizer_grouped_parameters = [\n",
    "#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "# optimizer = AdamW(optimizer_grouped_parameters,lr = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "classifier.weight torch.Size([5, 768])\n",
      "classifier.bias torch.Size([5])\n",
      "******************************\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "classifier.weight torch.Size([5, 768])\n",
      "classifier.bias torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "### 冻结部分参数\n",
    "\n",
    "unfreeze_layers = ['layer.11','bert.pooler','classifier.']\n",
    " \n",
    "for name, param in bert_softmax.named_parameters():\n",
    "    print(name,param.size())\n",
    "\n",
    "print(\"*\"*30)\n",
    "print('\\n')\n",
    "\n",
    "for name ,param in bert_softmax.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    for ele in unfreeze_layers:\n",
    "        if ele in name:\n",
    "            param.requires_grad = True\n",
    "            break\n",
    "#验证一下\n",
    "for name, param in bert_softmax.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name,param.size())\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤掉requires_grad = False的参数\n",
    "#optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, bert_softmax.parameters()), lr=0.1)\n",
    "lr = 0.0001\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, bert_softmax.parameters()), lr=lr,weight_decay = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(bert_softmax.parameters(), lr = 0.0001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(640.1537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.7000  batch_recall:0.0279  batch_f1: 0.0536\n",
      "bor_precision:0.0000  bor_recall:0.0000  bor_f1: 0.0000\n",
      "loss:  tensor(325.4315, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9517  batch_recall:0.5328  batch_f1: 0.6832\n",
      "bor_precision:0.0741  bor_recall:0.0632  bor_f1: 0.0682\n",
      "loss:  tensor(172.5672, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.7603  batch_recall:0.8675  batch_f1: 0.8104\n",
      "bor_precision:0.4737  bor_recall:0.6667  bor_f1: 0.5538\n",
      "loss:  tensor(85.1098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.7610  batch_recall:0.9695  batch_f1: 0.8527\n",
      "bor_precision:0.5149  bor_recall:0.7536  bor_f1: 0.6118\n",
      "loss:  tensor(73.8272, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9138  batch_recall:0.9593  batch_f1: 0.9360\n",
      "bor_precision:0.7882  bor_recall:0.8933  bor_f1: 0.8375\n",
      "loss:  tensor(25.8160, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9480  batch_recall:0.9591  batch_f1: 0.9535\n",
      "bor_precision:0.9483  bor_recall:0.9322  bor_f1: 0.9402\n",
      "loss:  tensor(24.1427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9600  batch_recall:0.9767  batch_f1: 0.9683\n",
      "bor_precision:0.8806  bor_recall:0.9219  bor_f1: 0.9008\n",
      "loss:  tensor(54.3386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9042  batch_recall:0.9353  batch_f1: 0.9195\n",
      "bor_precision:0.7674  bor_recall:0.8462  bor_f1: 0.8049\n",
      "loss:  tensor(20.6272, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9472  batch_recall:0.9831  batch_f1: 0.9648\n",
      "bor_precision:0.8750  bor_recall:0.9655  bor_f1: 0.9180\n",
      "loss:  tensor(50.2486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9144  batch_recall:0.9553  batch_f1: 0.9344\n",
      "bor_precision:0.7902  bor_recall:0.8898  bor_f1: 0.8370\n",
      "loss:  tensor(18.3803, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9194  batch_recall:0.9870  batch_f1: 0.9520\n",
      "bor_precision:0.8675  bor_recall:0.9351  bor_f1: 0.9000\n",
      "loss:  tensor(40.7451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9681  batch_recall:0.9785  batch_f1: 0.9733\n",
      "bor_precision:0.9200  bor_recall:0.9583  bor_f1: 0.9388\n",
      "loss:  tensor(11.1768, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9500  batch_recall:0.9967  batch_f1: 0.9728\n",
      "bor_precision:0.8938  bor_recall:0.9806  bor_f1: 0.9352\n",
      "loss:  tensor(13.2692, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9477  batch_recall:1.0000  batch_f1: 0.9732\n",
      "bor_precision:0.8966  bor_recall:0.9811  bor_f1: 0.9369\n",
      "loss:  tensor(39.2914, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9110  batch_recall:0.9808  batch_f1: 0.9446\n",
      "bor_precision:0.8462  bor_recall:0.9519  bor_f1: 0.8959\n",
      "loss:  tensor(11.1430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9518  batch_recall:1.0000  batch_f1: 0.9753\n",
      "bor_precision:0.9043  bor_recall:1.0000  bor_f1: 0.9497\n",
      "loss:  tensor(6.0289, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "batch_precision:0.9744  batch_recall:1.0000  batch_f1: 0.9870\n",
      "bor_precision:0.9333  bor_recall:1.0000  bor_f1: 0.9655\n",
      "0：结束\n"
     ]
    }
   ],
   "source": [
    "bert_softmax.train()\n",
    "for e in range(1):\n",
    "    count = 0\n",
    "    if (e+1)%2 == 0:\n",
    "        lr = lr/2\n",
    "        optimizer = AdamW(filter(lambda p: p.requires_grad, bert_softmax.parameters()), lr=lr,weight_decay = 0.1)\n",
    "    \n",
    "    for step, batch in enumerate(train_iter):\n",
    "        count += 1\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch   ##(batch_size,512)\n",
    "        input_ids = input_ids.cuda()\n",
    "        input_mask = input_mask.cuda()\n",
    "        segment_ids = segment_ids.cuda()\n",
    "        label_ids = label_ids.cuda()\n",
    "        out = bert_softmax(input_ids,segment_ids,input_mask)  #[batch_size,seq_len+2(512)]\n",
    "#         break\n",
    "        out1 = out.view(-1, out.size(2))\n",
    "        res1 = label_ids[0]\n",
    "        for i in range(1,len(label_ids)):\n",
    "            res1 = torch.cat((res1,label_ids[i]))\n",
    "        loss = criterion(out1,res1)\n",
    "        loss = loss/len(out)\n",
    "        \n",
    "        if count %5 == 0:\n",
    "            print('loss: ',loss)\n",
    "            out2 = torch.argmax(out,dim=2)\n",
    "            batch_precision,batch_recall,batch_f1 = evaluate(out2,label_ids,input_mask)\n",
    "            print('batch_precision:%.4f  batch_recall:%.4f  batch_f1: %.4f' %(batch_precision,batch_recall,batch_f1)) \n",
    "            \n",
    "            bor_precision, bor_recall,bor_f1 = evaluate_bor(out2,label_ids,input_mask)\n",
    "            print('bor_precision:%.4f  bor_recall:%.4f  bor_f1: %.4f' %(bor_precision,bor_recall,bor_f1)) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(str(e)+'：结束')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_bor(out2,label_ids,input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## batch_precision:0.9526  batch_recall:0.9731  batch_f1: 0.9628\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_entitys(label_id):\n",
    "#     valid_pos = torch.nonzero(label_id).squeeze(1)\n",
    "#     entitys = []\n",
    "#     #         entitys2 = []\n",
    "#     flag = valid_pos[0]\n",
    "#     start = flag\n",
    "#     for vp in valid_pos[1:]:\n",
    "#         if vp -1 == flag:\n",
    "#             end = vp\n",
    "#         else:\n",
    "#             entity = [start.item(),(end+1).item()]\n",
    "#     #         entity = entity.replace(' ','')\n",
    "#             entitys.append(entity)\n",
    "#             start = vp\n",
    "\n",
    "#         flag = vp\n",
    "#     entity = [start.item(),(end+1).item()]\n",
    "#     # entity = entity.replace(' ','')\n",
    "#     entitys.append(entity)   ### 最后一个实体\n",
    "#     return entitys\n",
    "\n",
    "# get_entitys(label_ids[0])\n",
    "\n",
    "# out2 = torch.argmax(out,dim=2)\n",
    "# get_entitys(out2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 保存模型\n",
    "# len(bert_softmax.state_dict())\n",
    "PATH = '../model_save/bert_softmax'\n",
    "torch.save(bert_softmax.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load model\n",
    "config = BertConfig.from_json_file('../bert-base-chinese/bert_config.json')\n",
    "# bert_softmax = Bert_Softmax(config,5).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-Nh', 'I-Nh', 'B-NDR', 'I-NDR']\n",
    "eval_iter = create_batch_iter(\"train\",'../multi_data/multi_testner.txt',8,label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load_model\n",
    "PATH = '../model_save/bert_softmax'\n",
    "model = Bert_Softmax(config,5).cuda()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "\n",
    "#### load_model \n",
    "# out = model(input_ids,segment_ids,input_mask)  \n",
    "        \n",
    "# out1 = out.view(-1, out.size(2))\n",
    "# res1 = label_ids[0]\n",
    "# for i in range(1,len(label_ids)):\n",
    "#     res1 = torch.cat((res1,label_ids[i]))\n",
    "# loss = criterion(out1,res1)\n",
    "# loss = loss/len(out)\n",
    "\n",
    "# if count %5 == 0:\n",
    "# print('loss: ',loss)\n",
    "# out2 = torch.argmax(out,dim=2)\n",
    "# batch_precision,batch_recall,batch_f1 = evaluate(out2,label_ids,input_mask)\n",
    "# print('batch_precision:%.4f  batch_recall:%.4f  batch_f1: %.4f' %(batch_precision,batch_recall,batch_f1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [10.0] * len(label_list)   #生成一维\n",
    "\n",
    "weight[0] = 1\n",
    "\n",
    "weight = torch.tensor(weight).cuda()\n",
    "\n",
    "criterion = nn.NLLLoss(weight,size_average=False)      #定义的损失函数，softmax以后求损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iter = create_batch_iter(\"train\",'../multi_data/multi_testner.txt',8,label_list)\n",
    "# bert_softmax.train()\n",
    "for step, batch in enumerate(eval_iter):\n",
    "#     count += 1\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch   ##(batch_size,512)\n",
    "    input_ids = input_ids.cuda()\n",
    "    input_mask = input_mask.cuda()\n",
    "    segment_ids = segment_ids.cuda()\n",
    "    label_ids = label_ids.cuda()\n",
    "    out = model(input_ids,segment_ids,input_mask)  #[batch_size,seq_len+2(512)]\n",
    "\n",
    "    out1 = out.view(-1, out.size(2))\n",
    "    res1 = label_ids[0]\n",
    "    for i in range(1,len(label_ids)):\n",
    "        res1 = torch.cat((res1,label_ids[i]))\n",
    "    loss = criterion(out1,res1)\n",
    "    loss = loss/len(out)\n",
    "\n",
    "    \n",
    "    print('loss: ',loss)\n",
    "    out2 = torch.argmax(out,dim=2)\n",
    "    batch_precision,batch_recall,batch_f1 = evaluate(out2,label_ids,input_mask)\n",
    "    print('batch_precision:%.4f  batch_recall:%.4f  batch_f1: %.4f' %(batch_precision,batch_recall,batch_f1)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(input_mask[3].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(label_ids[3].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(input_ids[3].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #     label_true = []\n",
    "    #     y_pre = []\n",
    "#         loss = torch.tensor(0.0,requires_grad=True)\n",
    "#         for i in range(len(out)):\n",
    "#     #         label_true.append([label_ids[i][j].item() for j in range(512) if input_mask[i][j] == 1 ])\n",
    "#     #         y_pre.append([out[i][j].tolist() for j in range(512) if input_mask[i][j] == 1 ])\n",
    "            \n",
    "#             label_true = [label_ids[i][j].item() for j in range(512) if input_mask[i][j] == 1 ]\n",
    "#             y_pre = [out[i][j].tolist() for j in range(512) if input_mask[i][j] == 1 ]\n",
    "\n",
    "#             assert len(label_true) == len(y_pre)\n",
    "\n",
    "#             label_true = torch.tensor(label_true).cuda()\n",
    "#             y_pre = torch.tensor(y_pre).cuda()\n",
    "\n",
    "#             tmp_loss = criterion(y_pre, label_true)\n",
    "# #             tmp_loss = tmp_loss/len(y_pre)\n",
    "#             loss = loss+tmp_loss\n",
    "# #             loss.backward()\n",
    "# #             optimizer.backward(loss)\n",
    "    \n",
    "    \n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pipeline_middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import MyPro,convert_examples_to_features,create_batch_iter\n",
    "from bert_ner import Bert_Softmax\n",
    "# from bert_ner import Bert_Softmax\n",
    "import torch\n",
    "from transformers import BertConfig,AdamW\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from Evalutor import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-Nh', 'I-Nh', 'B-NDR', 'I-NDR']\n",
    "# train_iter = create_batch_iter(\"train\",'../multi_data/multi_trainner.txt',12,label_list)\n",
    "\n",
    "config = BertConfig.from_json_file('../bert-base-chinese/bert_config.json')\n",
    "bert_softmax = Bert_Softmax(config,5).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../model_save/bert_softmax'\n",
    "model = Bert_Softmax(config,5).cuda()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [10.0] * len(label_list)   #生成一维\n",
    "\n",
    "weight[0] = 1\n",
    "\n",
    "weight = torch.tensor(weight).cuda()\n",
    "\n",
    "criterion = nn.NLLLoss(weight,size_average=False)      #定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('../bert-base-chinese/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_iter = create_batch_iter(\"train\",'../multi_data/multi_testner.txt',8,label_list)\n",
    "# bert_softmax.train()\n",
    "lines = []\n",
    "train_iter = create_batch_iter(\"train\",'../multi_data/multi_trainner.txt',12,label_list)\n",
    "for step, batch in enumerate(train_iter):\n",
    "#     y_pres = []\n",
    "#     count += 1\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch   ##(batch_size,512)\n",
    "    input_ids = input_ids.cuda()\n",
    "    input_mask = input_mask.cuda()\n",
    "    segment_ids = segment_ids.cuda()\n",
    "    label_ids = label_ids.cuda()\n",
    "    out = model(input_ids,segment_ids,input_mask)  #[batch_size,seq_len+2(512)]\n",
    "    \n",
    "    out = torch.argmax(out,dim= 2)\n",
    "    \n",
    "    for i in range(len(out)):\n",
    "        label_true = [label_ids[i][j].item() for j in range(512) if input_mask[i][j] == 1 ]\n",
    "        y_pre = [out[i][j].tolist() for j in range(512) if input_mask[i][j] == 1 ]\n",
    "        y_pre = torch.tensor(y_pre)\n",
    "        valid_pos = torch.nonzero(y_pre).squeeze(1)\n",
    "        \n",
    "        entitys = []\n",
    "#         entitys2 = []\n",
    "        flag = valid_pos[0]\n",
    "        start = flag\n",
    "        for vp in valid_pos[1:]:\n",
    "            if vp -1 == flag:\n",
    "                end = vp\n",
    "            else:\n",
    "                entity = tokenizer.decode(input_ids[i][start:end+1])\n",
    "                entity = entity.replace(' ','')\n",
    "                entitys.append(entity)\n",
    "                start = vp\n",
    "        \n",
    "            flag = vp\n",
    "        entity = tokenizer.decode(input_ids[i][start:valid_pos[-1]+1])\n",
    "        entity = entity.replace(' ','')\n",
    "        entitys.append(entity)   ### 最后一个实体\n",
    "#         print(entitys)\n",
    "        context = tokenizer.decode(input_ids[i][:len(y_pre)][1:-1])\n",
    "        context = context.replace(' ','')\n",
    "#         context = context[1:-1]\n",
    "        \n",
    "        \n",
    "        entitys = list(set(entitys))   ### 去重，无所谓顺序，因为两两实体都会进行组合\n",
    "#         e1_e2 = []\n",
    "        e_len = len(entitys)\n",
    "        for i in range(e_len):\n",
    "            e1 = entitys[i]\n",
    "            for j in range(e_len):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                e2 = entitys[j]\n",
    "                line = e1+'，'+e2+'。'+context\n",
    "                lines.append(line)\n",
    "#                 print(line)\n",
    "#                 e1_e2.append([e1,e2])\n",
    "\n",
    "                \n",
    "#         print(e1_e2)\n",
    "#         print(context)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         y_pres.append()\n",
    "        \n",
    "#         break\n",
    "        \n",
    "    \n",
    "    \n",
    "#     break\n",
    "\n",
    "    '''out1 = out.view(-1, out.size(2))\n",
    "    res1 = label_ids[0]\n",
    "    for i in range(1,len(label_ids)):\n",
    "        res1 = torch.cat((res1,label_ids[i]))\n",
    "    loss = criterion(out1,res1)\n",
    "    loss = loss/len(out)\n",
    "\n",
    "    \n",
    "    print('loss: ',loss)\n",
    "    out2 = torch.argmax(out,dim=2)\n",
    "    batch_precision,batch_recall,batch_f1 = evaluate(out2,label_ids,input_mask)\n",
    "    print('batch_precision:%.4f  batch_recall:%.4f  batch_f1: %.4f' %(batch_precision,batch_recall,batch_f1)) '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/tmp.txt','r',encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line for line in lines if len(json.loads(line))<=510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_lines = []\n",
    "for line in lines:\n",
    "    t = {}\n",
    "    t['source'] = json.loads(line)\n",
    "    t['target'] = 'O'\n",
    "    re_lines.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(re_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "with open('tmp/tmp2.txt','w+',encoding = 'utf-8') as f:\n",
    "    for line in tqdm(re_lines):\n",
    "        line_str = json.dumps(line)\n",
    "        f.write(line_str+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from redata_loader import MyPro,convert_examples_to_features,create_batch_iter\n",
    "from redata_loader import create_batch_iter\n",
    "from bert_ner import RE_Bert_Softmax\n",
    "# from bert_ner import Bert_Softmax\n",
    "import torch\n",
    "from transformers import BertConfig,AdamW\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from Evalutor import re_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {}\n",
    "tag2id['O'] = 0\n",
    "tag2id['traffic_in'] = 1\n",
    "tag2id['sell_drugs_to'] = 2\n",
    "tag2id['provide_shelter_for'] = 3\n",
    "tag2id['posess'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = create_batch_iter(\"train\",'../multi_data/multi_trainre.txt',100,tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file('../bert-base-chinese/bert_config.json')\n",
    "bert_softmax = RE_Bert_Softmax(config,5).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irsdc/songwenhui/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# weight = [10.0] * len(tag2id)   #生成一维\n",
    "# weight[0] = 1\n",
    "# weight = torch.tensor(weight).cuda()\n",
    "\n",
    "criterion = nn.NLLLoss(size_average=False)      #定义的损失函数，softmax以后求损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight torch.Size([21128, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "classifier.weight torch.Size([5, 768])\n",
      "classifier.bias torch.Size([5])\n",
      "******************************\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "classifier.weight torch.Size([5, 768])\n",
      "classifier.bias torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "### 冻结部分参数\n",
    "\n",
    "unfreeze_layers = ['layer.11','bert.pooler','classifier.']\n",
    " \n",
    "for name, param in bert_softmax.named_parameters():\n",
    "    print(name,param.size())\n",
    "\n",
    "print(\"*\"*30)\n",
    "print('\\n')\n",
    "\n",
    "for name ,param in bert_softmax.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    for ele in unfreeze_layers:\n",
    "        if ele in name:\n",
    "            param.requires_grad = True\n",
    "            break\n",
    "#验证一下\n",
    "for name, param in bert_softmax.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤掉requires_grad = False的参数\n",
    "#optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, bert_softmax.parameters()), lr=0.1)\n",
    "lr = 0.0002\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, bert_softmax.parameters()), lr=lr,weight_decay = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.4595  batch_recall:0.2931  batch_f1: 0.3579\n",
      "loss:  tensor(1.4726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.0000  batch_recall:0.0000  batch_f1: 0.0000\n",
      "loss:  tensor(1.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6000  batch_recall:0.3000  batch_f1: 0.4000\n",
      "loss:  tensor(1.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6429  batch_recall:0.4655  batch_f1: 0.5400\n",
      "loss:  tensor(0.9801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.5882  batch_recall:0.4918  batch_f1: 0.5357\n",
      "loss:  tensor(0.8568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7333  batch_recall:0.8209  batch_f1: 0.7746\n",
      "loss:  tensor(0.8205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7042  batch_recall:0.7812  batch_f1: 0.7407\n",
      "0：结束\n",
      "loss:  tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7969  batch_recall:0.7727  batch_f1: 0.7846\n",
      "loss:  tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6883  batch_recall:0.8983  batch_f1: 0.7794\n",
      "loss:  tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.8393  batch_recall:0.7460  batch_f1: 0.7899\n",
      "loss:  tensor(0.7071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6528  batch_recall:0.8868  batch_f1: 0.7520\n",
      "loss:  tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7612  batch_recall:0.8226  batch_f1: 0.7907\n",
      "loss:  tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7237  batch_recall:0.8871  batch_f1: 0.7971\n",
      "loss:  tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.8286  batch_recall:0.8169  batch_f1: 0.8227\n",
      "1：结束\n",
      "loss:  tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7273  batch_recall:0.9492  batch_f1: 0.8235\n",
      "loss:  tensor(0.5636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7000  batch_recall:0.8596  batch_f1: 0.7717\n",
      "loss:  tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7031  batch_recall:0.7627  batch_f1: 0.7317\n",
      "loss:  tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7683  batch_recall:0.9545  batch_f1: 0.8514\n",
      "loss:  tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6914  batch_recall:0.9492  batch_f1: 0.8000\n",
      "loss:  tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7467  batch_recall:0.8889  batch_f1: 0.8116\n",
      "loss:  tensor(0.4791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7571  batch_recall:0.8983  batch_f1: 0.8217\n",
      "2：结束\n",
      "loss:  tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7436  batch_recall:0.9206  batch_f1: 0.8227\n",
      "loss:  tensor(0.5422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6618  batch_recall:0.9000  batch_f1: 0.7627\n",
      "loss:  tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.8028  batch_recall:0.8143  batch_f1: 0.8085\n",
      "loss:  tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.8312  batch_recall:0.8889  batch_f1: 0.8591\n",
      "loss:  tensor(0.4658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7532  batch_recall:0.9667  batch_f1: 0.8467\n",
      "loss:  tensor(0.4409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7927  batch_recall:0.9559  batch_f1: 0.8667\n",
      "loss:  tensor(0.4899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7867  batch_recall:0.9365  batch_f1: 0.8551\n",
      "3：结束\n",
      "loss:  tensor(0.4289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.8095  batch_recall:0.9714  batch_f1: 0.8831\n",
      "loss:  tensor(0.5182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7763  batch_recall:0.8806  batch_f1: 0.8252\n",
      "loss:  tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6389  batch_recall:0.8846  batch_f1: 0.7419\n",
      "loss:  tensor(0.5330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6716  batch_recall:0.8182  batch_f1: 0.7377\n",
      "loss:  tensor(0.4651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7945  batch_recall:0.9062  batch_f1: 0.8467\n",
      "loss:  tensor(0.5202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7143  batch_recall:0.9836  batch_f1: 0.8276\n",
      "loss:  tensor(0.4261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7692  batch_recall:0.9524  batch_f1: 0.8511\n",
      "4：结束\n",
      "loss:  tensor(0.5018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7237  batch_recall:0.8871  batch_f1: 0.7971\n",
      "loss:  tensor(0.5133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6575  batch_recall:0.9231  batch_f1: 0.7680\n",
      "loss:  tensor(0.4808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7349  batch_recall:0.9531  batch_f1: 0.8299\n",
      "loss:  tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7013  batch_recall:1.0000  batch_f1: 0.8244\n",
      "loss:  tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7353  batch_recall:0.8475  batch_f1: 0.7874\n",
      "loss:  tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6875  batch_recall:0.9483  batch_f1: 0.7971\n",
      "loss:  tensor(0.4266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7927  batch_recall:0.9155  batch_f1: 0.8497\n",
      "5：结束\n",
      "loss:  tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.8551  batch_recall:0.9077  batch_f1: 0.8806\n",
      "loss:  tensor(0.4154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.8193  batch_recall:0.9444  batch_f1: 0.8774\n",
      "loss:  tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6892  batch_recall:0.9107  batch_f1: 0.7846\n",
      "loss:  tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7703  batch_recall:0.9500  batch_f1: 0.8507\n",
      "loss:  tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.6716  batch_recall:0.8036  batch_f1: 0.7317\n",
      "loss:  tensor(0.3952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7808  batch_recall:0.9661  batch_f1: 0.8636\n",
      "loss:  tensor(0.4726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7500  batch_recall:0.9844  batch_f1: 0.8514\n",
      "6：结束\n",
      "loss:  tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.5571  batch_recall:0.8478  batch_f1: 0.6724\n",
      "loss:  tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7385  batch_recall:0.8276  batch_f1: 0.7805\n",
      "loss:  tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7875  batch_recall:0.9130  batch_f1: 0.8456\n",
      "loss:  tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.8354  batch_recall:0.9565  batch_f1: 0.8919\n",
      "loss:  tensor(0.4441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7368  batch_recall:0.9825  batch_f1: 0.8421\n",
      "loss:  tensor(0.4594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.7470  batch_recall:0.9394  batch_f1: 0.8322\n",
      "loss:  tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_precision:0.8356  batch_recall:0.9385  batch_f1: 0.8841\n",
      "7：结束\n"
     ]
    }
   ],
   "source": [
    "bert_softmax.train()\n",
    "for e in range(8):\n",
    "    count = 0\n",
    "    if (e+1)%2 == 0:\n",
    "        lr = lr/2\n",
    "        optimizer = AdamW(filter(lambda p: p.requires_grad, bert_softmax.parameters()), lr=lr,weight_decay = 0.1)\n",
    "    \n",
    "    for step, batch in enumerate(train_iter):\n",
    "        count += 1\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch   ##(batch_size,512)\n",
    "        input_ids = input_ids.cuda()\n",
    "        input_mask = input_mask.cuda()\n",
    "        segment_ids = segment_ids.cuda()\n",
    "        label_ids = label_ids.cuda()\n",
    "        out = bert_softmax(input_ids,segment_ids,input_mask)  #[batch_size,seq_len+2(512)]\n",
    "        \n",
    "        loss = F.cross_entropy(out, label_ids)\n",
    "#         break\n",
    "        \n",
    "#         out1 = out.view(-1, out.size(2))\n",
    "#         res1 = label_ids[0]\n",
    "#         for i in range(1,len(label_ids)):\n",
    "#             res1 = torch.cat((res1,label_ids[i]))\n",
    "#         loss = criterion(out,label_ids)\n",
    "        #loss = loss/len(out)\n",
    "        \n",
    "        if count %5 == 0:\n",
    "            print('loss: ',loss)\n",
    "#             out2 = torch.argmax(out,dim=2)\n",
    "            batch_precision,batch_recall,batch_f1 = re_evaluate(out,label_ids)\n",
    "            print('batch_precision:%.4f  batch_recall:%.4f  batch_f1: %.4f' %(batch_precision,batch_recall,batch_f1)) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(str(e)+'：结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.00 0.8642 0.9167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 保存模型\n",
    "# len(bert_softmax.state_dict())\n",
    "PATH = '../model_save/re_bert_softmax'\n",
    "torch.save(bert_softmax.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline 之RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from redata_loader import MyPro,convert_examples_to_features,create_batch_iter\n",
    "from redata_loader import create_batch_iter\n",
    "from bert_ner import RE_Bert_Softmax\n",
    "# from bert_ner import Bert_Softmax\n",
    "import torch\n",
    "from transformers import BertConfig,AdamW\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from Evalutor import re_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {}\n",
    "tag2id['O'] = 0\n",
    "tag2id['traffic_in'] = 1\n",
    "tag2id['sell_drugs_to'] = 2\n",
    "tag2id['provide_shelter_for'] = 3\n",
    "tag2id['posess'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tag = []\n",
    "id2tag.append('O')\n",
    "id2tag.append('traffic_in')\n",
    "id2tag.append('sell_drugs_to')\n",
    "id2tag.append('provide_shelter_for')\n",
    "id2tag.append('posess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = create_batch_iter(\"train\",'tmp/tmp2.txt',16,tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file('../bert-base-chinese/bert_config.json')\n",
    "bert_softmax = RE_Bert_Softmax(config,5).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../model_save/re_bert_softmax'\n",
    "rmodel = RE_Bert_Softmax(config,5).cuda()\n",
    "rmodel.load_state_dict(torch.load(PATH))\n",
    "rmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('../bert-base-chinese/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lines = []\n",
    "for step, batch in enumerate(train_iter):\n",
    "#     count += 1\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch   ##(batch_size,512)\n",
    "    input_ids = input_ids.cuda()\n",
    "    input_mask = input_mask.cuda()\n",
    "    segment_ids = segment_ids.cuda()\n",
    "    label_ids = label_ids.cuda()\n",
    "    out = rmodel(input_ids,segment_ids,input_mask)  #[batch_size,seq_len+2(512)]\n",
    "    out = torch.softmax(out,dim=1)\n",
    "    out = torch.argmax(out,dim = 1)\n",
    "    for i in range(len(out)):\n",
    "        label_true = [1 for j in range(512) if input_mask[i][j] == 1 ] ## 检验有效长度\n",
    "        length = len(label_true)\n",
    "        tag = id2tag[out[i]]\n",
    "        context = tokenizer.decode(input_ids[i][1:length-1])\n",
    "        context = context.replace(' ','')\n",
    "        res_line = {}\n",
    "        res_line['source'] = context\n",
    "        res_line['target'] = tag\n",
    "        \n",
    "        res_lines.append(res_line)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "with open('tmp/result.txt','w+',encoding = 'utf-8') as f:\n",
    "    for line in tqdm(res_lines):\n",
    "        line_str = json.dumps(line)\n",
    "        f.write(line_str+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline 最后预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evalutor import pipe_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepath = '../multi_data/'\n",
    "filename = 'multi_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prepath+filename,'r',encoding='gbk') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer('../bert-base-chinese/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_lines = []\n",
    "# re_lines = []\n",
    "# labels = []\n",
    "true_infor = {}\n",
    "for line in lines:\n",
    "    ner_line = {}\n",
    "    con = json.loads(line)\n",
    "    sentText = con['sentText']\n",
    "    relationMentions = con['relationMentions']\n",
    "    triples = []\n",
    "    tuples = []\n",
    "    entitys = []\n",
    "#     e1s = []\n",
    "#     e2s = []\n",
    "    for relationMention in relationMentions:\n",
    "        e1 = relationMention['em1Text']\n",
    "        e2 = relationMention['em2Text']\n",
    "        label = relationMention['label']\n",
    "        triples.append((label,e1,e2))\n",
    "        tuples.append((e1,e2))\n",
    "        entitys.append(e1)\n",
    "        entitys.append(e2)\n",
    "        \n",
    "    senttokens = list(sentText)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(senttokens)\n",
    "    sentText = tokenizer.decode(input_ids)\n",
    "    sentText = sentText.replace(' ','')\n",
    "    true_infor[sentText] = triples\n",
    "#     break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_infor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/result.txt','r',encoding='gbk') as f:\n",
    "    mlines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etotal = [0.0,0.0,0.0,0.0]  ### tp,tn,fp,fn\n",
    "for line in mlines:\n",
    "    line_dict = json.loads(line)\n",
    "    source = line_dict['source']\n",
    "    label = line_dict['target']\n",
    "    so_list = source.split('。',1)\n",
    "    so1 = so_list[1].replace('[[UNK][UNK][UNK]]','[UNK]')\n",
    "    \n",
    "    if so1 not in true_infor:\n",
    "        continue\n",
    "    Tlabels = true_infor[so1]\n",
    "#     print(Tlabels)   ### e.g. [('posess', '欧某某', '甲基苯丙胺'), ('posess', '欧某某', '麻黄碱'), ('posess', '欧某某', '巴比妥')]\n",
    "    \n",
    "    so_0 = so_list[0]\n",
    "    e1_e2 = so_0.split('，')\n",
    "    pre_infor = (label,e1_e2[0],e1_e2[1])\n",
    "#     print(pre_infor)   ###e.g. ('posess', '欧某某', '甲基苯丙胺')\n",
    "    \n",
    "    eva = pipe_evaluate(pre_infor,Tlabels)\n",
    "    etotal = [ii+jj for ii,jj in zip(etotal,eva)]\n",
    "#     print(etotal)\n",
    "#     break\n",
    "    \n",
    "#     aa = aa.replace('[[UNK][UNK][UNK]]','[UNK]')\n",
    "#     if so1 not in true_infor:\n",
    "#         print(so1)\n",
    "    \n",
    "    \n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp,tn,fp,fn = etotal\n",
    "p = tp/(tp+fp)\n",
    "r = tp/(tp+fn)\n",
    "f1 = 2*p*r/(p+r)\n",
    "print('precision: ',tp/(tp+fp))   ## 0.2984\n",
    "print('recall: ',tp/(tp+fn))    ## 0.9885\n",
    "print('f1: ',f1)                ## 0.4542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in true_infor.items():\n",
    "    if k[:9] == '同年11月3日10时许，':\n",
    "        k1 = k\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
